{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SimplySnap/PDLP-AMD-RIPS/blob/connorphillips700-patch-1/PDHG_torch_pulp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1YwT7icPYHwF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YwT7icPYHwF",
        "outputId": "fc15e4ca-77a3-42e7-ceb1-b22bfe75b689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pulp\n",
            "  Downloading pulp-3.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Downloading pulp-3.2.1-py3-none-any.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-3.2.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pulp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "js4-ygh3YB8N",
      "metadata": {
        "id": "js4-ygh3YB8N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pulp import LpProblem, LpConstraintEQ, LpConstraintGE, LpConstraintLE, LpAffineExpression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ikCl3zgLWhTw",
      "metadata": {
        "id": "ikCl3zgLWhTw"
      },
      "outputs": [],
      "source": [
        "def project_lambda_box(grad, is_neg_inf, is_pos_inf):\n",
        "    \"\"\"\n",
        "    Projects the gradient onto the normal cone of the feasible region defined by bounds l and u.\n",
        "\n",
        "    For each i:\n",
        "      - If l[i] == -inf and u[i] == +inf: projection is 0\n",
        "      - If l[i] == -inf and u[i] is real: clamp to ≤ 0 (R⁻)\n",
        "      - If l[i] is real and u[i] == +inf: clamp to ≥ 0 (R⁺)\n",
        "      - If both are finite: no projection (keep full value)\n",
        "\n",
        "    Args:\n",
        "        grad: (n, 1) gradient vector (torch tensor)\n",
        "        l: (n, 1) lower bounds (torch tensor)\n",
        "        u: (n, 1) upper bounds (torch tensor)\n",
        "\n",
        "    Returns:\n",
        "        projected: (n, 1) projected gradient (interpreted as λ)\n",
        "    \"\"\"\n",
        "    projected = torch.zeros_like(grad)\n",
        "\n",
        "    # Case 1: (-inf, +inf) → {0}\n",
        "    unconstrained = is_neg_inf & is_pos_inf\n",
        "    projected[unconstrained] = 0.0\n",
        "\n",
        "    # Case 2: (-inf, real) → R⁻ → clamp at 0 from above\n",
        "    neg_only = is_neg_inf & ~is_pos_inf\n",
        "    projected[neg_only] = torch.clamp(grad[neg_only], max=0.0)\n",
        "\n",
        "    # Case 3: (real, +inf) → R⁺ → clamp at 0 from below\n",
        "    pos_only = ~is_neg_inf & is_pos_inf\n",
        "    projected[pos_only] = torch.clamp(grad[pos_only], min=0.0)\n",
        "\n",
        "    # Case 4: (real, real) → full space → keep gradient\n",
        "    fully_bounded = ~is_neg_inf & ~is_pos_inf\n",
        "    projected[fully_bounded] = grad[fully_bounded]\n",
        "\n",
        "    return projected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "T8Rjc3crqwiw",
      "metadata": {
        "id": "T8Rjc3crqwiw"
      },
      "outputs": [],
      "source": [
        "def pdhg_torch(c, G, h, A, b, l, u, is_neg_inf, is_pos_inf, l_dual, u_dual, device, max_iter=1000, tol=1e-4, verbose=True):\n",
        "    \"\"\"\n",
        "    Solves:\n",
        "        min cᵀx s.t. Gx ≥ h, Ax = b, l ≤ x ≤ u\n",
        "    using the Primal-Dual Hybrid Gradient (PDHG) algorithm.\n",
        "    \"\"\"\n",
        "    n = c.shape[0]\n",
        "    m_ineq = G.shape[0] if G.numel() > 0 else 0\n",
        "    m_eq = A.shape[0] if A.numel() > 0 else 0\n",
        "\n",
        "    # Combine constraints\n",
        "    combined_matrix_list = []\n",
        "    rhs = []\n",
        "    if m_ineq > 0:\n",
        "        combined_matrix_list.append(G)\n",
        "        rhs.append(h)\n",
        "    if m_eq > 0:\n",
        "        combined_matrix_list.append(A)\n",
        "        rhs.append(b)\n",
        "\n",
        "    if not combined_matrix_list:\n",
        "        raise ValueError(\"Both G and A matrices are empty.\")\n",
        "\n",
        "    K = torch.vstack(combined_matrix_list).to(device)           # Combined constraint matrix\n",
        "    q = torch.vstack(rhs).to(device)                            # Combined right-hand side\n",
        "    c = c.to(device)\n",
        "\n",
        "    q_norm = torch.linalg.norm(q, 2)\n",
        "    c_norm = torch.linalg.norm(c, 2)\n",
        "\n",
        "\n",
        "    eta = 0.9 / torch.linalg.norm(K, 2)\n",
        "    omega = 1.0\n",
        "\n",
        "    tau = eta / omega\n",
        "    sigma = eta * omega\n",
        "\n",
        "    theta = 1.0\n",
        "\n",
        "    # Initialize primal and dual\n",
        "    x = torch.zeros((n, 1), device=device)\n",
        "    x_old = x.clone()\n",
        "    y = torch.zeros((K.shape[0], 1), device=device)\n",
        "\n",
        "    for k in range(max_iter):\n",
        "        x_old.copy_(x)\n",
        "\n",
        "        # Compute gradient and primal update\n",
        "        Kt_y = K.T @ y\n",
        "        grad = c - Kt_y\n",
        "        x = torch.clamp(x - tau * grad, min=l, max=u)\n",
        "\n",
        "        # Extrapolate\n",
        "        x_bar = x + theta * (x - x_old)\n",
        "\n",
        "        # Dual update\n",
        "        K_xbar = K @ x_bar\n",
        "        y += sigma * (q - K_xbar)\n",
        "\n",
        "        # Project duals:\n",
        "        if m_ineq > 0:\n",
        "            y[:m_ineq] = torch.clamp(y[:m_ineq], min=0.0)\n",
        "\n",
        "        # --- Check Termination Every 10 Iterations ---\n",
        "        if k % 10 == 0:\n",
        "            # Primal and dual objective\n",
        "            prim_obj = (c.T @ x)[0][0]\n",
        "            dual_obj = (q.T @ y)[0][0]\n",
        "\n",
        "            # Lagrange multipliers from box projection\n",
        "            lam = project_lambda_box(grad, is_neg_inf, is_pos_inf)\n",
        "            lam_pos = (l_dual.T @ torch.clamp(lam, min=0.0))[0][0]\n",
        "            lam_neg = (u_dual.T @ torch.clamp(lam, max=0.0))[0][0]\n",
        "\n",
        "            adjusted_dual = dual_obj + lam_pos + lam_neg\n",
        "            duality_gap = abs(adjusted_dual - prim_obj)\n",
        "\n",
        "            # Primal residual (feasibility)\n",
        "            residual_eq = A @ x - b if m_eq > 0 else torch.zeros(1, device=device)\n",
        "            residual_ineq = torch.clamp(h - G @ x, min=0.0) if m_ineq > 0 else torch.zeros(1, device=device)\n",
        "            primal_residual = torch.norm(torch.vstack([residual_eq, residual_ineq]), p=2).item()\n",
        "\n",
        "            # Dual residual (change in x)\n",
        "            dual_residual = torch.norm(grad - lam, p=2).item()\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"[{k}] Primal Obj: {prim_obj:.4f}, Adjusted Dual Obj: {adjusted_dual:.4f}, \"\n",
        "                      f\"Gap: {duality_gap:.2e}, Prim Res: {primal_residual:.2e}, Dual Res: {dual_residual:.2e}\")\n",
        "\n",
        "            # Termination condition\n",
        "            if (primal_residual <= tol * (1 + q_norm) and\n",
        "                dual_residual <= tol * (1 + c_norm) and\n",
        "                duality_gap <= tol * (1 + abs(prim_obj) + abs(adjusted_dual))):\n",
        "                if verbose:\n",
        "                    print(f\"Converged at iteration {k}\")\n",
        "                break\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "EGSh4cqjjIro",
      "metadata": {
        "id": "EGSh4cqjjIro"
      },
      "outputs": [],
      "source": [
        "def mps_to_standard_form_torch(mps_file, device='cpu'):\n",
        "    \"\"\"\n",
        "    Reads an MPS file and converts the linear programming problem into standard form tensors.\n",
        "    The standard form is:\n",
        "    minimize     c^T * x\n",
        "    subject to   G * x >= h\n",
        "                 A * x = b\n",
        "                 l <= x <= u\n",
        "    \"\"\"\n",
        "    # fromMPS returns the problem and a dictionary mapping variable names to LpVariable objects\n",
        "    _, prob = LpProblem.fromMPS(mps_file)\n",
        "    var_list = prob.variables()\n",
        "    num_vars = len(var_list)\n",
        "    name_to_idx = {var.name: i for i, var in enumerate(var_list)}\n",
        "\n",
        "    # --- Bounds (l, u) ---\n",
        "    l = [-np.inf if v.lowBound is None else v.lowBound for v in var_list]\n",
        "    u = [np.inf if v.upBound is None else v.upBound for v in var_list]\n",
        "\n",
        "    # --- Objective (c) ---\n",
        "    c = np.array([prob.objective.get(v, 0.0) for v in var_list])\n",
        "\n",
        "    # --- Constraints (A, b, G, h) ---\n",
        "    A_rows, G_rows = [], []\n",
        "    b_eq, b_ineq = [], []\n",
        "\n",
        "    for con in prob.constraints.values():\n",
        "        row = np.zeros(num_vars)\n",
        "        for var, coeff in con.items():\n",
        "          idx = name_to_idx[var.name]\n",
        "          row[idx] = coeff\n",
        "\n",
        "        # PuLP stores the RHS constant on the left side, so we negate it\n",
        "        rhs = -con.constant\n",
        "\n",
        "        if con.sense == LpConstraintEQ:\n",
        "            # Ax = b\n",
        "            A_rows.append(row)\n",
        "            b_eq.append(rhs)\n",
        "        elif con.sense == LpConstraintGE:\n",
        "            # Gx >= h\n",
        "            G_rows.append(row)\n",
        "            b_ineq.append(rhs)\n",
        "        elif con.sense == LpConstraintLE:\n",
        "            # Gx <= h\n",
        "            G_rows.append(-row)\n",
        "            b_ineq.append(-rhs)\n",
        "\n",
        "    # --- Convert to PyTorch Tensors ---\n",
        "    c_tensor = torch.tensor(c, dtype=torch.float32, device=device).view(-1, 1)\n",
        "    l_tensor = torch.tensor(l, dtype=torch.float32, device=device).view(-1, 1)\n",
        "    u_tensor = torch.tensor(u, dtype=torch.float32, device=device).view(-1, 1)\n",
        "\n",
        "    A_tensor = torch.tensor(A_rows, dtype=torch.float32, device=device) if A_rows else torch.zeros((0, num_vars), device=device)\n",
        "    b_tensor = torch.tensor(b_eq, dtype=torch.float32, device=device).view(-1, 1) if b_eq else torch.zeros((0, 1), device=device)\n",
        "\n",
        "    G_tensor = torch.tensor(G_rows, dtype=torch.float32, device=device) if G_rows else torch.zeros((0, num_vars), device=device)\n",
        "    h_tensor = torch.tensor(b_ineq, dtype=torch.float32, device=device).view(-1, 1) if b_ineq else torch.zeros((0, 1), device=device)\n",
        "\n",
        "    return c_tensor, G_tensor, h_tensor, A_tensor, b_tensor, l_tensor, u_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ab18afcd-3581-4876-89c4-e4cd5bd15dc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab18afcd-3581-4876-89c4-e4cd5bd15dc1",
        "outputId": "5bf68a93-68f8-4ba6-dbd3-f0694002a5b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROCm/CUDA not available. PyTorch is using CPU.\n",
            "[0] Primal Obj: -46.0888, Adjusted Dual Obj: 148.7681, Gap: 1.95e+02, Prim Res: 1.81e+01, Dual Res: 9.00e+00\n",
            "[10] Primal Obj: 53.7725, Adjusted Dual Obj: 54.8024, Gap: 1.03e+00, Prim Res: 3.57e-02, Dual Res: 4.11e-01\n",
            "[20] Primal Obj: 54.0354, Adjusted Dual Obj: 53.9620, Gap: 7.35e-02, Prim Res: 3.94e-03, Dual Res: 2.30e-03\n",
            "[30] Primal Obj: 53.9995, Adjusted Dual Obj: 54.0003, Gap: 7.51e-04, Prim Res: 7.52e-05, Dual Res: 1.37e-04\n",
            "Converged at iteration 30\n",
            "\n",
            "Solution (first 10 variables):\n",
            "[[ 4.      ]\n",
            " [-1.      ]\n",
            " [ 5.999947]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nf/r2gt2wsd5pnb2cqvrbrw75t40000gn/T/ipykernel_19254/1472275374.py:54: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_e2rjusv8ek/croot/pytorch-select_1717607459930/work/torch/csrc/utils/tensor_new.cpp:277.)\n",
            "  A_tensor = torch.tensor(A_rows, dtype=torch.float32, device=device) if A_rows else torch.zeros((0, num_vars), device=device)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    # --- Configuration ---\n",
        "    mps_file_path = 'sample.mps'\n",
        "\n",
        "    # --- Device Selection ---\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"PyTorch is using ROCm/CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"ROCm/CUDA not available. PyTorch is using CPU.\")\n",
        "\n",
        "    # --- Data Loading ---\n",
        "    try:\n",
        "        c, G, h, A, b, l, u = mps_to_standard_form_torch(mps_file_path, device=device)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load MPS file: {e}\")\n",
        "        exit(1)\n",
        "\n",
        "    is_neg_inf = torch.isinf(l) & (l < 0)\n",
        "    is_pos_inf = torch.isinf(u) & (u > 0)\n",
        "\n",
        "    l_dual = l.clone()\n",
        "    u_dual = u.clone()\n",
        "\n",
        "    l_dual[is_neg_inf] = 0\n",
        "    u_dual[is_pos_inf] = 0\n",
        "\n",
        "    # --- Run PDHG Solver on the GPU or CPU ---\n",
        "    solution = pdhg_torch(c, G, h, A, b, l, u, is_neg_inf, is_pos_inf, l_dual, u_dual, device=device)\n",
        "\n",
        "    print(\"\\nSolution (first 10 variables):\")\n",
        "    print(solution[:10].cpu().numpy())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cosc410",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
