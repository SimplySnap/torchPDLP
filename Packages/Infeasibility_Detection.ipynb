{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Jj81yeOcVsH9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def ruiz_precondition(c, K, q, l, u, device='cpu', max_iter=20, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Performs Ruiz equilibration (scaling) on the standard-form linear program using GPU tensors.\n",
    "\n",
    "    This is done to improve the numerical stability of iterative solvers, especially for\n",
    "    ill-conditioned problems.\n",
    "\n",
    "    Standard form of the LP:\n",
    "        minimize     cᵀx\n",
    "        subject to   Gx ≥ h\n",
    "                     Ax = b\n",
    "                     l ≤ x ≤ u\n",
    "\n",
    "    Inputs:\n",
    "    -------\n",
    "    c  : (n x 1) torch tensor — objective function vector\n",
    "    K  : ((m_ineq + m_eq) x n) torch tensor — constraint matrix (stacked G and A)\n",
    "    q  : ((m_ineq + m_eq) x 1) torch tensor — RHS vector (stacked h and b)\n",
    "    l  : (n x 1) torch tensor — lower bounds on variables\n",
    "    u  : (n x 1) torch tensor — upper bounds on variables\n",
    "    max_iter : int — number of scaling iterations to perform (default: 20)\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "    K_s : ((m_ineq + m_eq) x n) torch tensor — scaled constraint matrix (stacked G and A)\n",
    "    c_s : (n x 1) torch tensor — scaled objective vector\n",
    "    q_s : ((m_ineq + m_eq) x 1) torch tensor — scaled RHS vector (stacked h and b)\n",
    "    l_s : (n x 1) torch tensor — scaled lower bounds\n",
    "    u_s : (n x 1) torch tensor — scaled upper bounds\n",
    "    D_col : (n x 1) torch tensor — final column scaling factors (for rescaling solution)\n",
    "    m_ineq : int — number of inequality constraints (used for slicing G vs A in K_s if needed)\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The scaling preserves feasibility and optimality but improves numerical conditioning.\n",
    "    - You must rescale your solution after solving using D_col (and D_row if needed).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Scaling Loop ---\n",
    "    K_s, c_s, q_s, l_s, u_s = K.clone(), c.clone(), q.clone(), l.clone(), u.clone()\n",
    "    m, n = K_s.shape\n",
    "\n",
    "    D_row = torch.ones((m, 1), dtype=K.dtype, device=device)\n",
    "    D_col = torch.ones((n, 1), dtype=K.dtype, device=device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        row_norms = torch.sqrt(torch.linalg.norm(K_s, ord=torch.inf, dim=1, keepdim=True))\n",
    "        row_norms[row_norms < eps] = 1.0\n",
    "        D_row /= row_norms\n",
    "        K_s /= row_norms\n",
    "\n",
    "        col_norms = torch.sqrt(torch.linalg.norm(K_s, ord=torch.inf, dim=0, keepdim=True))\n",
    "        col_norms[col_norms < eps] = 1.0\n",
    "        D_col /= col_norms.T\n",
    "        K_s /= col_norms\n",
    "\n",
    "        if (torch.max(torch.abs(1 - row_norms)) < eps and\n",
    "            torch.max(torch.abs(1 - row_norms)) < eps):\n",
    "            break\n",
    "\n",
    "    c_s *= D_col\n",
    "    q_s *= D_row\n",
    "    l_s /= D_col\n",
    "    u_s /= D_col\n",
    "\n",
    "    return K_s, c_s, q_s, l_s, u_s, (D_col, D_row, K, c, q, l, u)\n",
    "\n",
    "def primal_weight_update(x_prev, x, y_prev, y, omega, smooth_theta):\n",
    "    diff_y_norm = torch.linalg.norm(y_prev - y, 2)\n",
    "    diff_x_norm = torch.linalg.norm(x_prev - x, 2)\n",
    "    if diff_x_norm > 0 and diff_y_norm > 0:\n",
    "        omega = torch.exp(smooth_theta * (torch.log(diff_y_norm/diff_x_norm)) + (1-smooth_theta)*torch.log(omega))\n",
    "    return omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OBoTtCThV0FA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def project_lambda_box(grad, is_neg_inf, is_pos_inf):\n",
    "    \"\"\"\n",
    "    Projects the gradient onto the normal cone of the feasible region defined by bounds l and u.\n",
    "\n",
    "    For each i:\n",
    "      - If l[i] == -inf and u[i] == +inf: projection is 0\n",
    "      - If l[i] == -inf and u[i] is real: clamp to ≤ 0 (R⁻)\n",
    "      - If l[i] is real and u[i] == +inf: clamp to ≥ 0 (R⁺)\n",
    "      - If both are finite: no projection (keep full value)\n",
    "\n",
    "    Args:\n",
    "        grad: (n, 1) gradient vector (torch tensor)\n",
    "        l: (n, 1) lower bounds (torch tensor)\n",
    "        u: (n, 1) upper bounds (torch tensor)\n",
    "\n",
    "    Returns:\n",
    "        projected: (n, 1) projected gradient (interpreted as λ)\n",
    "    \"\"\"\n",
    "    projected = torch.zeros_like(grad)\n",
    "\n",
    "    # Case 1: (-inf, +inf) → {0}\n",
    "    unconstrained = is_neg_inf & is_pos_inf\n",
    "    projected[unconstrained] = 0.0\n",
    "\n",
    "    # Case 2: (-inf, real) → R⁻ → clamp at 0 from above\n",
    "    neg_only = is_neg_inf & ~is_pos_inf\n",
    "    projected[neg_only] = torch.clamp(grad[neg_only], max=0.0)\n",
    "\n",
    "    # Case 3: (real, +inf) → R⁺ → clamp at 0 from below\n",
    "    pos_only = ~is_neg_inf & is_pos_inf\n",
    "    projected[pos_only] = torch.clamp(grad[pos_only], min=0.0)\n",
    "\n",
    "    # Case 4: (real, real) → full space → keep gradient\n",
    "    fully_bounded = ~is_neg_inf & ~is_pos_inf\n",
    "    projected[fully_bounded] = grad[fully_bounded]\n",
    "\n",
    "    return projected\n",
    "\n",
    "def spectral_norm_estimate_torch(K, num_iters=10):\n",
    "  \"\"\"\n",
    "  Estimates the spectral norm of a matrix K with enough acuracy to use in\n",
    "  setting the step size of the PDHG algorithm.\n",
    "  \"\"\"\n",
    "\n",
    "  b = torch.randn(K.shape[1], 1, device=K.device)\n",
    "  for _ in range(num_iters):\n",
    "      b = K.T @ (K @ b)\n",
    "      b /= torch.norm(b)\n",
    "  return torch.norm(K @ b)\n",
    "\n",
    "def compute_residuals_and_duality_gap(x, y, c, q, K, m_ineq, is_neg_inf, is_pos_inf, l_dual, u_dual):\n",
    "    \"\"\"\n",
    "    Computes the primal and dual residuals, duality gap, and KKT error.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Primal variable.\n",
    "        y (torch.Tensor): Dual variable.\n",
    "        c (torch.Tensor): Coefficients for the primal objective.\n",
    "        q (torch.Tensor): Right-hand side vector for the constraints.\n",
    "        K (torch.Tensor): Constraint matrix.\n",
    "        m_ineq (int): Number of inequality constraints.\n",
    "        omega (float): Scaling factor for the dual update.\n",
    "        is_neg_inf (torch.Tensor): Boolean mask for negative infinity lower bounds.\n",
    "        is_pos_inf (torch.Tensor): Boolean mask for positive infinity upper bounds.\n",
    "        l_dual (torch.Tensor): Lower bounds for the dual variables.\n",
    "        u_dual (torch.Tensor): Upper bounds for the dual variables.\n",
    "    Returns:\n",
    "        primal_residual (torch.Tensor): Norm of the primal residual.\n",
    "        dual_residual (torch.Tensor): Norm of the dual residual.\n",
    "        duality_gap (torch.Tensor): Duality gap.\n",
    "    \"\"\"\n",
    "    # Primal and dual objective\n",
    "    grad = c - K.T @ y\n",
    "    prim_obj = (c.T @ x).flatten()\n",
    "    dual_obj = (q.T @ y).flatten()\n",
    "\n",
    "    # Lagrange multipliers from box projection\n",
    "    lam = project_lambda_box(grad, is_neg_inf, is_pos_inf)\n",
    "    lam_pos = (l_dual.T @ torch.clamp(lam, min=0.0)).flatten()\n",
    "    lam_neg = (u_dual.T @ torch.clamp(lam, max=0.0)).flatten()\n",
    "\n",
    "    adjusted_dual = dual_obj + lam_pos + lam_neg\n",
    "    duality_gap = adjusted_dual - prim_obj\n",
    "\n",
    "    # Primal residual (feasibility)\n",
    "    full_residual = K @ x - q\n",
    "    residual_ineq = torch.clamp(full_residual[:m_ineq], max=0.0)\n",
    "    residual_eq = full_residual[m_ineq:]\n",
    "    primal_residual = torch.norm(torch.vstack([residual_eq, residual_ineq]), p=2).flatten()\n",
    "\n",
    "    # Dual residual (change in x)\n",
    "    dual_residual = torch.norm(grad - lam, p=2).flatten()\n",
    "\n",
    "    return primal_residual, dual_residual, duality_gap, prim_obj, adjusted_dual\n",
    "\n",
    "def KKT_error(x, y, c, q, K, m_ineq, omega, is_neg_inf, is_pos_inf, l_dual, u_dual, device):\n",
    "      \"\"\"\n",
    "      Computes the KKT error using global variables.\n",
    "      \"\"\"\n",
    "      omega_sqrd = omega ** 2\n",
    "      # Compute primal and dual residuals, and duality gap\n",
    "      primal_residual, dual_residual, duality_gap, _ , _ = compute_residuals_and_duality_gap(x, y, c, q, K, m_ineq, is_neg_inf, is_pos_inf, l_dual, u_dual)\n",
    "      # Compute the error\n",
    "      KKT = torch.sqrt(omega_sqrd * primal_residual ** 2 + (dual_residual ** 2) / omega_sqrd + duality_gap ** 2)\n",
    "\n",
    "      return KKT\n",
    "\n",
    "def check_termination(primal_residual, dual_residual, duality_gap, prim_obj, adjusted_dual, q_norm, c_norm, tol):\n",
    "    \"\"\"\n",
    "    Checks the termination conditions for the PDHG algorithm.\n",
    "    Args:\n",
    "        primal_residual (torch.Tensor): Norm of the primal residual.\n",
    "        dual_residual (torch.Tensor): Norm of the dual residual.\n",
    "        duality_gap (torch.Tensor): Duality gap.\n",
    "        prim_obj (torch.Tensor): Primal objective value.\n",
    "        adjusted_dual (torch.Tensor): Adjusted dual objective value.\n",
    "        q_norm (float): Norm of the right-hand side vector q.\n",
    "        c_norm (float): Norm of the coefficients vector c.\n",
    "        tol (float): Tolerance for stopping criterion.\n",
    "    Returns:\n",
    "        bool: True if termination conditions are met, False otherwise.\n",
    "    \"\"\"\n",
    "    cond1 = primal_residual <= tol * (1 + q_norm)\n",
    "    cond2 = dual_residual <= tol * (1 + c_norm)\n",
    "    cond3 = duality_gap <= tol * (1 + abs(prim_obj) + abs(adjusted_dual))\n",
    "    return cond1 and cond2 and cond3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GU_zquTPV7PZ",
    "outputId": "b105753a-88f6-4975-f144-aa05bc40f483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using ROCm/CUDA device: AMD Instinct MI210\n",
      "\n",
      "Processing PDLP-AMD-RIPS/datasets/Netlib/infeasible/bgdbg1.mps...\n",
      "Artificial restart at iteration 40 using the Current iterate.\n",
      "[40] Primal Obj: 0.0000, Adjusted Dual Obj: 116.6342, Gap: 9.91e-01, Prim Res: 8.06e-02, Dual Res: 1.02e-02\n",
      "\n",
      "Artificial restart at iteration 40 using the Average iterate.\n",
      "[80] Primal Obj: 0.0000, Adjusted Dual Obj: 454.1176, Gap: 9.98e-01, Prim Res: 7.12e-02, Dual Res: 1.33e-02\n",
      "\n",
      "Artificial restart at iteration 80 using the Average iterate.\n",
      "[160] Primal Obj: 0.0000, Adjusted Dual Obj: 716.5501, Gap: 9.99e-01, Prim Res: 7.01e-02, Dual Res: 1.77e-03\n",
      "\n",
      "Artificial restart at iteration 120 using the Average iterate.\n",
      "[280] Primal Obj: 0.0000, Adjusted Dual Obj: 1134.0616, Gap: 9.99e-01, Prim Res: 7.00e-02, Dual Res: 2.89e-04\n",
      "\n",
      "Artificial restart at iteration 160 using the Average iterate.\n",
      "[440] Primal Obj: 0.0881, Adjusted Dual Obj: 1695.9414, Gap: 9.99e-01, Prim Res: 7.00e-02, Dual Res: 2.92e-04\n",
      "\n",
      "Artificial restart at iteration 280 using the Average iterate.\n",
      "[720] Primal Obj: 1.0299, Adjusted Dual Obj: 2675.1409, Gap: 9.99e-01, Prim Res: 6.99e-02, Dual Res: 2.29e-04\n",
      "\n",
      "Artificial restart at iteration 440 using the Average iterate.\n",
      "[1160] Primal Obj: 1.8437, Adjusted Dual Obj: 4209.9668, Gap: 9.99e-01, Prim Res: 6.99e-02, Dual Res: 1.46e-04\n",
      "\n",
      "Artificial restart at iteration 680 using the Average iterate.\n",
      "[1840] Primal Obj: 2.0391, Adjusted Dual Obj: 6579.4424, Gap: 9.99e-01, Prim Res: 6.99e-02, Dual Res: 1.45e-04\n",
      "\n",
      "Artificial restart at iteration 1040 using the Average iterate.\n",
      "[2880] Primal Obj: 4.2616, Adjusted Dual Obj: 10200.4434, Gap: 9.99e-01, Prim Res: 6.99e-02, Dual Res: 1.19e-04\n",
      "\n",
      "Artificial restart at iteration 1640 using the Average iterate.\n",
      "[4520] Primal Obj: 8.3851, Adjusted Dual Obj: 15906.6357, Gap: 9.99e-01, Prim Res: 6.99e-02, Dual Res: 6.25e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Instead of this\n",
    "# args = parse_args()\n",
    "\n",
    "# Do this manually\n",
    "class Args:\n",
    "    device = 'gpu'\n",
    "    instance_path = 'PDLP-AMD-RIPS/datasets/Netlib/infeasible'\n",
    "    tolerance = 1e-8\n",
    "    output_path = '/content/PDLP-AMD-RIPS/results'\n",
    "    precondition = True\n",
    "    primal_weight_update = False\n",
    "    adaptive_stepsize = False\n",
    "    verbose = False\n",
    "    support_sparse = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Run LP solver with configuration options.')\n",
    "\n",
    "    parser.add_argument('--device', type=str, choices=['cpu', 'gpu', 'auto'], default='auto',\n",
    "                        help=\"Device to run on: 'cpu', 'gpu', or 'auto' (default: auto)\")\n",
    "    parser.add_argument('--instance_path', type=str, default='feasible',\n",
    "                        help=\"Path to folder containing MPS instances (default: 'feasible')\")\n",
    "    parser.add_argument('--tolerance', type=float, default=1e-2,\n",
    "                        help=\"Tolerance for stopping criterion (default: 1e-2)\")\n",
    "    parser.add_argument('--output_path', type=str, default='output',\n",
    "                        help=\"Directory where outputs will be saved (default: 'output')\")\n",
    "    parser.add_argument('--precondition', action='store_true',\n",
    "                        help=\"Enable Ruiz preconditioning (default: False)\")\n",
    "    parser.add_argument('--primal_weight_update', action='store_true',\n",
    "                        help=\"Enable primal weight update (default: False)\")\n",
    "    parser.add_argument('--adaptive_stepsize', action='store_true',\n",
    "                        help=\"Enable adaptive stepsize for PDLP (default: False)\")\n",
    "    parser.add_argument('--verbose', action='store_true',\n",
    "                        help=\"Enable verbose output (default: False)\")\n",
    "    parser.add_argument('--support_sparse', action='store_true',\n",
    "                        help=\"Support sparse matrices operations(default: False)\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = Args #parse_args()\n",
    "\n",
    "    # --- Device Selection ---\n",
    "    if args.device == 'auto' or args.device == 'gpu':\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            print(f\"PyTorch is using ROCm/CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print(\"ROCm/CUDA not available. PyTorch is using CPU.\")\n",
    "    else:\n",
    "        device = torch.device(args.device)\n",
    "        print(f\"PyTorch is using device: {device}\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    mps_folder_path = args.instance_path\n",
    "    tol = args.tolerance\n",
    "    output_path = args.output_path\n",
    "    precondition = args.precondition\n",
    "    primal_weight_update = args.primal_weight_update\n",
    "    adaptive_stepsize = args.adaptive_stepsize\n",
    "    verbose=args.verbose\n",
    "    support_sparse = args.support_sparse\n",
    "    results = []\n",
    "\n",
    "    # --- Get all MPS files from the folder ---\n",
    "    mps_files = sorted([f for f in os.listdir(mps_folder_path) if f.endswith('.mps')])\n",
    "\n",
    "    for mps_file in mps_files:\n",
    "        mps_file_path = os.path.join(mps_folder_path, mps_file)\n",
    "        print(f\"\\nProcessing {mps_file_path}...\")\n",
    "        try:\n",
    "            # --- Load problem ---\n",
    "            c, K, q, m_ineq, l, u= mps_to_standard_form(mps_file_path, device=device, support_sparse=support_sparse, verbose=verbose)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load MPS file: {mps_file_path}. Error: {e}\")\n",
    "            results.append({\n",
    "                'File': mps_file,\n",
    "                'Objective': 'N/A',\n",
    "                'Iterations (k)': 'N/A',\n",
    "                'Time (s)': 'N/A',\n",
    "                'Status': f'Failed to load: {e}'\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with Timer(\"Solve time\") as t:\n",
    "                # PRECONDITION\n",
    "                if precondition:\n",
    "                    K, c, q, l, u, dt_precond = ruiz_precondition(c, K, q, l, u, device = device)\n",
    "\n",
    "                x, prim_obj, k, n, j = pdlp_algorithm(K, m_ineq, c, q, l, u, device, max_iter=10000000, tol=tol, verbose=True, restart_period=40, precondition=precondition,primal_update=primal_weight_update, adaptive=adaptive_stepsize, data_precond=dt_precond)\n",
    "\n",
    "                print(f\"Objective value: {prim_obj:.4f}\")\n",
    "\n",
    "            time_elapsed = t.elapsed\n",
    "            print(f\"Took {time_elapsed:.4f} seconds.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Solver failed for {mps_file}. Error: {e}\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # --- Call your solver/main logic here ---\n",
    "    print(f\"Instance path: {mps_folder_path}\")\n",
    "    print(f\"Tolerance: {tol}\")\n",
    "    print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f_EJGzna6-p",
    "outputId": "a738ecb3-87a6-408b-b350-d9fcb6011e8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PDLP-AMD-RIPS'...\n",
      "remote: Enumerating objects: 1730, done.\u001b[K\n",
      "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
      "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
      "remote: Total 1730 (delta 61), reused 36 (delta 21), pack-reused 1631 (from 2)\u001b[K\n",
      "Receiving objects: 100% (1730/1730), 343.40 MiB | 24.71 MiB/s, done.\n",
      "Resolving deltas: 100% (910/910), done.\n",
      "Updating files: 100% (427/427), done.\n"
     ]
    }
   ],
   "source": [
    "!cd PDLP-AMD-RIPS\n",
    "!rm -rf PDLP-AMD-RIPS\n",
    "!git clone --branch main https://github.com/SimplySnap/PDLP-AMD-RIPS.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pdlp_algorithm(K, m_ineq, c, q, l, u, device, max_iter=100_000, tol=1e-4, verbose=True, restart_period=40, precondition=False, primal_update=False, adaptive=False, data_precond=None):\n",
    "    '''\n",
    "    Main PDLP algorithm implementation with integrated infeasibility detection.\n",
    "    Args:\n",
    "        K (torch.Tensor): Constraint matrix.\n",
    "        m_ineq (int): Number of inequality constraints.\n",
    "        c (torch.Tensor): Coefficients for the primal objective.\n",
    "        q (torch.Tensor): Right-hand side vector for the constraints.\n",
    "        l (torch.Tensor): Lower bounds for the primal variable.\n",
    "        u (torch.Tensor): Upper bounds for the primal variable.\n",
    "        device (torch.device): Device to run the algorithm on (CPU or GPU).\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        tol (float): Tolerance for convergence.\n",
    "        verbose (bool): Whether to print detailed output.\n",
    "        restart_period (int): Number of iterations between restarts.\n",
    "        precondition (bool): Whether to use preconditioning.\n",
    "        primal_update (bool): Whether to perform primal weight updates.\n",
    "        adaptive (bool): Whether to use adaptive stepsize.\n",
    "        data_precond (tuple): Preconditioned data (D_col, D_row, K_unscaled, c_unscaled, q_unscaled, l_unscaled, u_unscaled) if precondition is True.\n",
    "\n",
    "    Returns:\n",
    "        x (torch.Tensor): Optimal primal variable.\n",
    "        prim_obj (float): Optimal primal objective value.\n",
    "        k (int): Total number of iterations.\n",
    "        n (int): Number of restart loops.\n",
    "        j (int): KKT pass counter.\n",
    "    '''\n",
    "    # Recover G, A, h, b for infeasibility detection\n",
    "    m_eq = K.shape[0] - m_ineq\n",
    "    if m_ineq > 0:\n",
    "        G_mat = K[:m_ineq]\n",
    "        h = q[:m_ineq]\n",
    "    else:\n",
    "        G_mat = torch.zeros((0, K.shape[1]), device=device)\n",
    "        h = torch.zeros((0, 1), device=device)\n",
    "    if m_eq > 0:\n",
    "        A_mat = K[m_ineq:]\n",
    "        b = q[m_ineq:]\n",
    "    else:\n",
    "        A_mat = torch.zeros((0, K.shape[1]), device=device)\n",
    "        b = torch.zeros((0, 1), device=device)\n",
    "\n",
    "    # Dual bound masks\n",
    "    is_neg_inf = torch.isinf(l) & (l < 0)\n",
    "    is_pos_inf = torch.isinf(u) & (u > 0)\n",
    "\n",
    "    l_dual = l.clone()\n",
    "    u_dual = u.clone()\n",
    "    l_dual[is_neg_inf] = 0\n",
    "    u_dual[is_pos_inf] = 0\n",
    "\n",
    "    q_norm = torch.linalg.norm(q, 2)\n",
    "    c_norm = torch.linalg.norm(c, 2)\n",
    "\n",
    "    # Initial step-size\n",
    "    eta = 0.9 / spectral_norm_estimate_torch(K, num_iters=100)\n",
    "    omega = c_norm / q_norm if q_norm > 1e-6 and c_norm > 1e-6 else torch.tensor(1.0)\n",
    "\n",
    "    theta = 1.0\n",
    "\n",
    "    # Restart Parameters [Sufficient, Necessary, Artificial]\n",
    "    beta = [0.2, 0.8, 0.36]\n",
    "\n",
    "    # Initialize primal and dual\n",
    "    x = torch.zeros((c.shape[0], 1), device=device)\n",
    "    y = torch.zeros((K.shape[0], 1), device=device)\n",
    "\n",
    "    # Infeasibility detection parameters\n",
    "    tol_eq = 1e-2\n",
    "    tol_feas = 1e-2\n",
    "    tol_obj = 1e-2\n",
    "    tol_dual_eq = 1e-2\n",
    "    tol_cert = 1e-2\n",
    "\n",
    "    # Trackers for infeasibility detection\n",
    "    x_prev_iter = x.clone()\n",
    "    y_prev_iter = y.clone()\n",
    "    lam_prev = torch.zeros_like(x)\n",
    "    certificate_flag = None\n",
    "    certificate_iter = None\n",
    "\n",
    "    # Counters\n",
    "    n = 0 # Outer Loop Counter\n",
    "    k = 0 # Total Iteration Counter\n",
    "    j = 0 # Kkt pass Counter\n",
    "\n",
    "    # Initialize Previous KKT Error\n",
    "    KKT_first = 0 # The actual KKT error of the very first point doesn't matter since the artificial criteria will always hit anyway\n",
    "\n",
    "    # -------------- Outer Loop --------------\n",
    "    while k < max_iter:\n",
    "        t = 0 # Initialize inner iteration counter\n",
    "\n",
    "        # Initialize/Reset sums for averaging\n",
    "        x_eta_total = torch.zeros_like(x)\n",
    "        y_eta_total = torch.zeros_like(y)\n",
    "        eta_total = 0\n",
    "\n",
    "        # Initialize/Reset Previous restart point for primal weighting\n",
    "        x_last_restart = x.clone()\n",
    "        y_last_restart = y.clone()\n",
    "\n",
    "        # --------- Inner Loop ---------\n",
    "        while k < max_iter:\n",
    "            k += 1\n",
    "            x_previous = x.clone() # For checking necessary criteria\n",
    "            y_previous = y.clone()\n",
    "\n",
    "            if adaptive:\n",
    "                # Adaptive step of pdhg\n",
    "                x, y, eta, eta_hat, j = adaptive_one_step_pdhg(x, y, c, q, K, l, u, m_ineq, eta, omega, theta, k, j)\n",
    "            else:\n",
    "                # Fixed step of pdhg\n",
    "                x, y, eta, eta_hat = fixed_one_step_pdhg(x, y, c, q, K, l, u, m_ineq, eta, omega, theta)\n",
    "                j += 1\n",
    "\n",
    "            # Increase iteration counters\n",
    "            t += 1\n",
    "\n",
    "            # Update totals\n",
    "            x_eta_total += eta * x\n",
    "            y_eta_total += eta * y\n",
    "            eta_total += eta\n",
    "\n",
    "            # Update eta\n",
    "            eta = eta_hat\n",
    "\n",
    "            # Infeasibility detection\n",
    "            lam = project_lambda_box(c - K.T @ y, is_neg_inf, is_pos_inf)\n",
    "            if k > 1:  # Need at least two points to compute differences\n",
    "                dx = x - x_prev_iter\n",
    "                dy = y - y_prev_iter\n",
    "                dlam = lam - lam_prev\n",
    "\n",
    "                # Dual infeasibility (primal unbounded) detection\n",
    "                dlam_plus = (-dlam).clamp(min=0)\n",
    "                dlam_minus = dlam.clamp(min=0)\n",
    "\n",
    "                # Check dual infeasibility (primal unbounded)\n",
    "                if m_eq == 0 or (A_mat @ dx).norm() < tol_eq:\n",
    "                    Gdx = G_mat @ dx if m_ineq > 0 else torch.zeros((1, 1), device=device)\n",
    "                    if (m_ineq == 0 or torch.all(Gdx >= -tol_feas)) and (c.T @ dx < tol_obj):\n",
    "                        bounds_ok = True\n",
    "                        for i in range(x.shape[0]):\n",
    "                            dx_i = dx[i].item()\n",
    "                            c_i = c[i].item()\n",
    "                            l_i = l[i].item()\n",
    "                            u_i = u[i].item()\n",
    "                            if not (\n",
    "                                (not torch.isinf(l[i]) and not torch.isinf(u[i]) and abs(dx_i) <= tol_feas) or\n",
    "                                (u_i == float('inf') and c_i >= 0 and dx_i >= -tol_feas) or\n",
    "                                (l_i == -float('inf') and c_i <= 0 and dx_i <= tol_feas)\n",
    "                            ):\n",
    "                                bounds_ok = False\n",
    "                                break\n",
    "                        if bounds_ok:\n",
    "                            certificate_flag = \"DUAL_INFEASIBLE\"\n",
    "                            certificate_iter = k\n",
    "                            if verbose:\n",
    "                                print(f\"[PDLP] Dual infeasibility detected at iter {k}\")\n",
    "                            break\n",
    "\n",
    "                # Primal infeasibility (dual unbounded) detection\n",
    "                dy_in = dy[:m_ineq] if m_ineq > 0 else torch.zeros((0, 1), device=device)\n",
    "                dy_eq = dy[m_ineq:] if m_eq > 0 else torch.zeros((0, 1), device=device)\n",
    "                dual_res = torch.zeros_like(x)\n",
    "                if m_ineq > 0: dual_res += G_mat.T @ dy_in\n",
    "                if m_eq > 0: dual_res += A_mat.T @ dy_eq\n",
    "                dual_res -= dlam\n",
    "\n",
    "                if dual_res.norm() < tol_dual_eq and (m_ineq == 0 or torch.all(dy_in >= -tol_feas)):\n",
    "                    dual_combo = 0.0\n",
    "                    if m_ineq > 0: dual_combo += (h.T @ dy_in).item()\n",
    "                    if m_eq > 0: dual_combo += (b.T @ dy_eq).item()\n",
    "\n",
    "                    finite_l = (~torch.isinf(l).view(-1)) & (l.view(-1) != 0)\n",
    "                    finite_u = (~torch.isinf(u).view(-1)) & (u.view(-1) != 0)\n",
    "\n",
    "                    if finite_l.any():\n",
    "                        dual_combo -= (l[finite_l].view(1, -1) @ dlam_minus[finite_l].view(-1, 1)).item()\n",
    "                    if finite_u.any():\n",
    "                        dual_combo -= (u[finite_u].view(1, -1) @ dlam_plus[finite_u].view(-1, 1)).item()\n",
    "\n",
    "                    if dual_combo > -tol_cert:\n",
    "                        certificate_flag = \"PRIMAL_INFEASIBLE\"\n",
    "                        certificate_iter = k\n",
    "                        if verbose:\n",
    "                            print(f\"[PDLP] Primal infeasibility detected at iter {k}\")\n",
    "                        break\n",
    "\n",
    "            # Update previous iterates for next infeasibility check\n",
    "            x_prev_iter.copy_(x)\n",
    "            y_prev_iter.copy_(y)\n",
    "            lam_prev.copy_(lam)\n",
    "\n",
    "            # Check Restart Criteria Every restart_period iterations\n",
    "            if t % restart_period == 0:\n",
    "                # Compute averages\n",
    "                x_avg = x_eta_total / eta_total\n",
    "                y_avg = y_eta_total / eta_total\n",
    "\n",
    "                # Compute KKT errors\n",
    "                KKT_current = KKT_error(x, y, c, q, K, m_ineq, omega, is_neg_inf, is_pos_inf, l_dual, u_dual, device)\n",
    "                KKT_average = KKT_error(x_avg, y_avg, c, q, K, m_ineq, omega, is_neg_inf, is_pos_inf, l_dual, u_dual, device)\n",
    "                KKT_min = min(KKT_current, KKT_average)\n",
    "                KKT_previous = KKT_error(x_previous, y_previous, c, q, K, m_ineq, omega, is_neg_inf, is_pos_inf, l_dual, u_dual, device)\n",
    "\n",
    "                # Add three kkt passes\n",
    "                j += 3\n",
    "\n",
    "                # Check Restart Criteria and update with Restart Candidate\n",
    "                if KKT_min <= beta[0] * KKT_first: # Sufficient Criteria\n",
    "                    if verbose:\n",
    "                        print(f\"Sufficient restart at iteration {t} using the\",\n",
    "                              \"Average iterate.\" if KKT_current >= KKT_average else \"Current iterate.\")\n",
    "                    (x, y) = (x_avg, y_avg) if KKT_current >= KKT_average else (x, y)\n",
    "                    break\n",
    "                elif KKT_min <= beta[1] * KKT_first and KKT_min > KKT_previous: # Necessary Criteria\n",
    "                    if verbose:\n",
    "                        print(f\"Necessary restart at iteration {t} using the\",\n",
    "                              \"Average iterate.\" if KKT_current >= KKT_average else \"Current iterate.\")\n",
    "                    (x, y) = (x_avg, y_avg) if KKT_current >= KKT_average else (x, y)\n",
    "                    break\n",
    "                elif t >= beta[2] * k: # Artificial Criteria\n",
    "                    if verbose:\n",
    "                        print(f\"Artificial restart at iteration {t} using the\",\n",
    "                              \"Average iterate.\" if KKT_current >= KKT_average else \"Current iterate.\")\n",
    "                    (x, y) = (x_avg, y_avg) if KKT_current >= KKT_average else (x, y)\n",
    "                    break\n",
    "\n",
    "        # ------------- End Inner Loop ------------\n",
    "\n",
    "        # Handle infeasibility certificate\n",
    "        if certificate_flag:\n",
    "            prim_obj = c.T @ x\n",
    "            if verbose:\n",
    "                print(f\"Terminating due to {certificate_flag} at iteration {k}\")\n",
    "            return x, prim_obj.cpu().item(), k, n, j\n",
    "\n",
    "        n += 1 # Increase restart loop counter\n",
    "\n",
    "        if primal_update: # Primal weight update\n",
    "            omega = primal_weight_update(x_last_restart, x, y_last_restart, y, omega, 0.5)\n",
    "\n",
    "        KKT_first = KKT_error(x, y, c, q, K, m_ineq, omega, is_neg_inf, is_pos_inf, l_dual, u_dual, device)\n",
    "        j += 1 # Add one kkt pass\n",
    "\n",
    "        # Compute primal and dual residuals, and duality gap\n",
    "        if precondition:\n",
    "            D_col, D_row, K_unscaled, c_unscaled, q_unscaled, l_unscaled, u_unscaled = data_precond\n",
    "            l_unscaled[is_neg_inf] = 0\n",
    "            u_unscaled[is_pos_inf] = 0\n",
    "            primal_residual, dual_residual, duality_gap, prim_obj, adjusted_dual = compute_residuals_and_duality_gap(\n",
    "                D_col * x, D_row * y, c_unscaled, q_unscaled, K_unscaled, m_ineq,\n",
    "                is_neg_inf, is_pos_inf, l_unscaled, u_unscaled\n",
    "            )\n",
    "        else:\n",
    "            primal_residual, dual_residual, duality_gap, prim_obj, adjusted_dual = compute_residuals_and_duality_gap(\n",
    "                x, y, c, q, K, m_ineq, is_neg_inf, is_pos_inf, l_dual, u_dual\n",
    "            )\n",
    "        j += 1 # Add one kkt pass\n",
    "\n",
    "        if verbose:\n",
    "            rel_gap = duality_gap.item() / (1 + abs(prim_obj.item()) + abs(adjusted_dual.item()))\n",
    "            rel_primal = primal_residual.item() / (1 + q_norm)\n",
    "            rel_dual = dual_residual.item() / (1 + c_norm)\n",
    "\n",
    "            print(f\"[{k}] Primal Obj: {prim_obj.item():.4f}, Adjusted Dual Obj: {adjusted_dual.item():.4f}, \"\n",
    "                  f\"Gap: {rel_gap:.2e}, Prim Res: {rel_primal:.2e}, Dual Res: {rel_dual:.2e}\")\n",
    "            print(\"\")\n",
    "\n",
    "        # Termination conditions\n",
    "        if check_termination(primal_residual, dual_residual, duality_gap, prim_obj, adjusted_dual, q_norm, c_norm, tol):\n",
    "            if verbose:\n",
    "                print(f\"Converged at iteration {k} after {n} restart loops\")\n",
    "            break\n",
    "\n",
    "    # ------------------- End Outer Loop ------------------------\n",
    "\n",
    "    return x, prim_obj.cpu().item(), k, n, j\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DGmDw6vhYlgP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def fixed_one_step_pdhg(x, y, c, q, K, l, u, m_ineq, eta, omega, theta):\n",
    "    \"\"\"\n",
    "    Perform one step of the Primal-Dual Hybrid Gradient (PDHG) algorithm.\n",
    "    Args:\n",
    "        x (torch.Tensor): Current primal variable.\n",
    "        y (torch.Tensor): Current dual variable.\n",
    "        c (torch.Tensor): Coefficients for the primal objective.\n",
    "        q (torch.Tensor): Right-hand side vector for the constraints.\n",
    "        K (torch.Tensor): Constraint matrix.\n",
    "        l (torch.Tensor): Lower bounds for the primal variable.\n",
    "        u (torch.Tensor): Upper bounds for the primal variable.\n",
    "        m_ineq (int): Number of inequality constraints.\n",
    "        eta (float): Step size for the primal update.\n",
    "        omega (float): Scaling factor for the dual update.\n",
    "        theta (float): Extrapolation parameter.\n",
    "    Returns:\n",
    "        x (torch.Tensor): Updated primal variable.\n",
    "        y (torch.Tensor): Updated dual variable.\n",
    "    \"\"\"\n",
    "    x_old = x.clone()\n",
    "\n",
    "    # Compute gradient and primal update\n",
    "    Kt_y = K.T @ y\n",
    "    grad = c - Kt_y\n",
    "    x = torch.clamp(x - eta / omega * grad, min=l, max=u)\n",
    "\n",
    "    # Extrapolate\n",
    "    x_bar = x + theta * (x - x_old)\n",
    "\n",
    "    # Dual update\n",
    "    K_xbar = K @ x_bar\n",
    "    y += eta * omega * (q - K_xbar)\n",
    "\n",
    "    # Project dual:\n",
    "    if m_ineq > 0:\n",
    "        y[:m_ineq] = torch.clamp(y[:m_ineq], min=0.0)\n",
    "\n",
    "    return x, y, eta, eta\n",
    "\n",
    "\n",
    "def adaptive_one_step_pdhg(x, y, c, q, K, l, u, m_ineq, eta, omega, theta, k, j):\n",
    "    \"\"\"\n",
    "    Perform one step of the Primal-Dual Hybrid Gradient (PDHG) algorithm with adaptive stepsize.\n",
    "    Args:\n",
    "        x (torch.Tensor): Current primal variable.\n",
    "        y (torch.Tensor): Current dual variable.\n",
    "        c (torch.Tensor): Coefficients for the primal objective.\n",
    "        q (torch.Tensor): Right-hand side vector for the constraints.\n",
    "        K (torch.Tensor): Constraint matrix.\n",
    "        l (torch.Tensor): Lower bounds for the primal variable.\n",
    "        u (torch.Tensor): Upper bounds for the primal variable.\n",
    "        m_ineq (int): Number of inequality constraints.\n",
    "        eta (float): Step size for the primal update.\n",
    "        omega (float): Scaling factor for the dual update.\n",
    "        theta (float): Extrapolation parameter.\n",
    "        k (int): Current iteration number.\n",
    "        j (int): Current KKT pass number.\n",
    "    Returns:\n",
    "        x (torch.Tensor): Updated primal variable.\n",
    "        y (torch.Tensor): Updated dual variable.\n",
    "    \"\"\"\n",
    "    x_old = x.clone()\n",
    "    y_old = y.clone()\n",
    "\n",
    "    # Primal update\n",
    "    Kt_y = K.T @ y_old\n",
    "    grad = c - Kt_y\n",
    "\n",
    "    for i in range(200):\n",
    "\n",
    "        # --- CURRENT STEPSIZE ---\n",
    "        tau = eta / omega\n",
    "        sigma = eta * omega\n",
    "\n",
    "        x = torch.clamp(x_old - tau * grad, min=l, max=u)\n",
    "\n",
    "        # Extrapolate\n",
    "        diff_x = x - x_old\n",
    "        x_bar = x + theta * diff_x\n",
    "\n",
    "        # Dual update\n",
    "        K_xbar = K @ x_bar\n",
    "        y = y_old + sigma * (q - K_xbar)\n",
    "\n",
    "        # Project duals:\n",
    "        if m_ineq > 0:\n",
    "            y[:m_ineq] = torch.clamp(y[:m_ineq], min=0.0)\n",
    "\n",
    "        diff_y = y - y_old\n",
    "\n",
    "        j += 1\n",
    "\n",
    "        # Calculate the denominator for the eta_bar update\n",
    "        denominator = 2 * (diff_y.T @ K @ diff_x)\n",
    "\n",
    "        # --- CALCULATE NEW STEP SIZES ---\n",
    "        if denominator != 0:\n",
    "            numerator = omega * (torch.linalg.norm(diff_x)**2) + (torch.linalg.norm(diff_y)**2) / omega\n",
    "            eta_bar = numerator / abs(denominator)\n",
    "            eta_prime_term1 = (1 - (k + 1)**(-0.3)) * eta_bar\n",
    "        else:\n",
    "            eta_bar = torch.tensor(float('inf'))\n",
    "            eta_prime_term1 = torch.tensor(float('inf'))\n",
    "\n",
    "        eta_prime_term2 = (1 + (k + 1)**(-0.6)) * eta\n",
    "        eta_prime = torch.min(eta_prime_term1, eta_prime_term2)\n",
    "\n",
    "        if eta <= eta_bar:\n",
    "            return x, y, eta.squeeze(), eta_prime.squeeze(), j\n",
    "\n",
    "        eta = eta_prime\n",
    "\n",
    "        return x, y, eta.squeeze(), eta.squeeze(), j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lya8Cf5LYnNM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "class Timer:\n",
    "  \"\"\"\n",
    "  Timer class to measure execution time of code blocks.\n",
    "  Usage:\n",
    "\n",
    "    with Timer(\"Label\"):\n",
    "        # Code block to be timed\n",
    "\n",
    "  Output:\n",
    "    Label: <time in seconds> seconds\n",
    "  \"\"\"\n",
    "  def __init__(self, label=\"Elapsed time\"):\n",
    "      self.label = label\n",
    "\n",
    "  def __enter__(self):\n",
    "      self.start = time.perf_counter()\n",
    "      return self\n",
    "\n",
    "  def __exit__(self, *args):\n",
    "      self.end = time.perf_counter()\n",
    "      self.elapsed = self.end - self.start\n",
    "      print(f\"{self.label}: {self.elapsed:.6f} seconds\")\n",
    "\n",
    "def sparse_vs_dense(A, device='cpu', kkt_passes=10):\n",
    "    \"\"\"\n",
    "    Benchmarks matrix-vector multiplication using dense and sparse formats.\n",
    "\n",
    "    Parameters:\n",
    "        A (torch.Tensor): 2D matrix (dense tensor)\n",
    "        device (str): 'cpu' or 'cuda'\n",
    "        kkt_passes (even int): Number of repetitions for matrix multiplication\n",
    "\n",
    "    Returns:\n",
    "        tensor A as either sparse or dense, which ever is faster\n",
    "    \"\"\"\n",
    "    assert A.dim() == 2, \"Input must be a 2D matrix\"\n",
    "    m, n = A.shape\n",
    "    A = A.to(device)\n",
    "\n",
    "     # Precompute random vectors for fair timing\n",
    "    vecs_n = [torch.randn(n, 1, device=device) for _ in range(kkt_passes // 2)]\n",
    "    vecs_m = [torch.randn(m, 1, device=device) for _ in range(kkt_passes // 2)]\n",
    "\n",
    "    # Dense timing\n",
    "    A_transpose = A.t()\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for vec_n, vec_m in zip(vecs_n, vecs_m):\n",
    "        _ = A @ vec_n\n",
    "        _ = A_transpose @ vec_m\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    dense_time = time.time() - start\n",
    "\n",
    "    # Sparse timing\n",
    "    A_sparse = A.to_sparse()\n",
    "    A_sparse_transpose = A_sparse.t()\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for vec_n, vec_m in zip(vecs_n, vecs_m):\n",
    "        _ = torch.sparse.mm(A_sparse, vec_n)\n",
    "        _ = torch.sparse.mm(A_sparse_transpose, vec_m)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    sparse_time = time.time() - start\n",
    "\n",
    "    return A_sparse if sparse_time < dense_time else A\n",
    "\n",
    "def mps_to_standard_form(mps_file, device='cpu', support_sparse=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Parses an MPS file and returns the standard form LP components as PyTorch tensors:\n",
    "        minimize     cᵀx\n",
    "        subject to   G x ≥ h\n",
    "                     A x = b\n",
    "                     l ≤ x ≤ u\n",
    "\n",
    "    Returns: c, G, h, A, b, l, u\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #Read MPS file\n",
    "    with open(mps_file, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip() and not line.startswith('*')]\n",
    "\n",
    "    section = None\n",
    "    row_types = {}\n",
    "    row_indices = {}\n",
    "    col_data = defaultdict(list)\n",
    "    rhs_data = {}\n",
    "    range_data = {}\n",
    "    bound_data = defaultdict(dict)\n",
    "\n",
    "    row_counter = 0\n",
    "    var_names = []\n",
    "    seen_vars = set()\n",
    "    obj_row_name = None\n",
    "\n",
    "    for line in lines:\n",
    "        if line == 'NAME' or line == 'ENDATA':\n",
    "            continue\n",
    "        elif line == 'ROWS':\n",
    "            section = 'ROWS'\n",
    "            continue\n",
    "        elif line == 'COLUMNS':\n",
    "            section = 'COLUMNS'\n",
    "            continue\n",
    "        elif line == 'RHS':\n",
    "            section = 'RHS'\n",
    "            continue\n",
    "        elif line == 'RANGES':\n",
    "            section = 'RANGES'\n",
    "            continue\n",
    "        elif line == 'BOUNDS':\n",
    "            section = 'BOUNDS'\n",
    "            continue\n",
    "\n",
    "        tokens = line.split()\n",
    "        if section == 'ROWS':\n",
    "            sense, row_name = tokens\n",
    "            row_types[row_name] = sense\n",
    "            row_indices[row_name] = row_counter\n",
    "            if sense == 'N':\n",
    "                obj_row_name = row_name\n",
    "            row_counter += 1\n",
    "\n",
    "        elif section == 'COLUMNS':\n",
    "            var_name = tokens[0]\n",
    "            if var_name not in seen_vars:\n",
    "                var_names.append(var_name)\n",
    "                seen_vars.add(var_name)\n",
    "            for i in range(1, len(tokens), 2):\n",
    "                row, val = tokens[i], float(tokens[i + 1])\n",
    "                col_data[var_name].append((row, val))\n",
    "\n",
    "        elif section == 'RHS':\n",
    "            for i in range(1, len(tokens), 2):\n",
    "                row, val = tokens[i], float(tokens[i + 1])\n",
    "                rhs_data[row] = val\n",
    "\n",
    "        elif section == 'RANGES':\n",
    "            for i in range(1, len(tokens), 2):\n",
    "                row, val = tokens[i], float(tokens[i + 1])\n",
    "                range_data[row] = val\n",
    "\n",
    "        elif section == 'BOUNDS':\n",
    "            bound_type, _, var_name = tokens[:3]\n",
    "            val = float(tokens[3]) if len(tokens) > 3 else None\n",
    "            if bound_type == 'LO':\n",
    "                bound_data[var_name]['lo'] = val\n",
    "            elif bound_type == 'UP':\n",
    "                bound_data[var_name]['up'] = val\n",
    "            elif bound_type == 'FX':\n",
    "                bound_data[var_name]['lo'] = val\n",
    "                bound_data[var_name]['up'] = val\n",
    "            elif bound_type == 'FR':\n",
    "                bound_data[var_name]['lo'] = 0.0\n",
    "                bound_data[var_name]['up'] = float('inf')\n",
    "\n",
    "    # Final variable ordering and index mapping\n",
    "    var_index = {v: i for i, v in enumerate(var_names)}\n",
    "    num_vars = len(var_names)\n",
    "\n",
    "    # Build objective vector c\n",
    "    c = np.zeros(num_vars)\n",
    "    for var, entries in col_data.items():\n",
    "        col_idx = var_index[var]\n",
    "        for row_name, val in entries:\n",
    "            if row_name == obj_row_name:\n",
    "                c[col_idx] = val\n",
    "\n",
    "    # Build row vectors from col_data\n",
    "    row_vectors = {row: np.zeros(num_vars) for row in row_types}\n",
    "    for var, entries in col_data.items():\n",
    "        col_idx = var_index[var]\n",
    "        for row_name, val in entries:\n",
    "            row_vectors[row_name][col_idx] = val\n",
    "\n",
    "     # Build A (equality) and G (inequality)\n",
    "    A_rows, b_eq = [], []\n",
    "    G_rows, h_ineq = [], []\n",
    "\n",
    "    for row_name, sense in row_types.items():\n",
    "        if row_name == obj_row_name:\n",
    "            continue\n",
    "\n",
    "        row_vec = row_vectors[row_name]\n",
    "        rhs_val = rhs_data.get(row_name, 0.0)\n",
    "        range_val = range_data.get(row_name, None)\n",
    "\n",
    "        if range_val is not None:\n",
    "            if sense == 'G':\n",
    "                lb = rhs_val\n",
    "                ub = rhs_val + abs(range_val)\n",
    "            elif sense == 'L':\n",
    "                ub = rhs_val\n",
    "                lb = rhs_val - abs(range_val)\n",
    "            elif sense == 'E':\n",
    "                if range_val > 0:\n",
    "                    lb = rhs_val\n",
    "                    ub = rhs_val + range_val\n",
    "                else:\n",
    "                    ub = rhs_val\n",
    "                    lb = rhs_val + range_val\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ranged sense: {sense}\")\n",
    "\n",
    "            G_rows.append(row_vec)\n",
    "            h_ineq.append(lb)\n",
    "            G_rows.append(-row_vec)\n",
    "            h_ineq.append(-ub)\n",
    "\n",
    "        else:\n",
    "            if sense == 'E':\n",
    "                A_rows.append(row_vec)\n",
    "                b_eq.append(rhs_val)\n",
    "            elif sense == 'G':\n",
    "                G_rows.append(row_vec)\n",
    "                h_ineq.append(rhs_val)\n",
    "            elif sense == 'L':\n",
    "                G_rows.append(-row_vec)\n",
    "                h_ineq.append(-rhs_val)\n",
    "\n",
    "    # Bounds\n",
    "    l = []\n",
    "    u = []\n",
    "    for var in var_names:\n",
    "        lo = bound_data[var].get('lo', 0)\n",
    "        up = bound_data[var].get('up', float('inf'))\n",
    "        l.append(lo)\n",
    "        u.append(up)\n",
    "\n",
    "    # Convert all to torch\n",
    "    A_tensor = torch.tensor(np.array(A_rows), dtype=torch.float32, device=device)\n",
    "    b_tensor = torch.tensor(np.array(b_eq), dtype=torch.float32, device=device).view(-1, 1)\n",
    "    G_tensor = torch.tensor(np.array(G_rows), dtype=torch.float32, device=device)\n",
    "    h_tensor = torch.tensor(np.array(h_ineq), dtype=torch.float32, device=device).view(-1, 1)\n",
    "    c_tensor = torch.tensor(c, dtype=torch.float32, device=device).view(-1, 1)\n",
    "    l_tensor = torch.tensor(l, dtype=torch.float32, device=device).view(-1, 1)\n",
    "    u_tensor = torch.tensor(u, dtype=torch.float32, device=device).view(-1, 1)\n",
    "\n",
    "    m_ineq = G_tensor.shape[0] if G_tensor.numel() > 0 else 0\n",
    "\n",
    "    # Combine original constraints into K and q\n",
    "    combined_matrix_list = []\n",
    "    rhs = []\n",
    "    if m_ineq > 0:\n",
    "        combined_matrix_list.append(G_tensor)\n",
    "        rhs.append(h_tensor)\n",
    "    if A_tensor.numel() > 0:\n",
    "        combined_matrix_list.append(A_tensor)\n",
    "        rhs.append(b_tensor)\n",
    "\n",
    "    K_tensor = torch.vstack(combined_matrix_list)\n",
    "    q_tensor = torch.vstack(rhs)\n",
    "\n",
    "    if support_sparse:\n",
    "        # Check if sparse operations are faster\n",
    "        K_tensor = sparse_vs_dense(K_tensor, device=device, kkt_passes=10)\n",
    "        if verbose:\n",
    "            print(\"Using Sparse operations\") if K_tensor.is_sparse else print(\"Using Dense operations\")\n",
    "\n",
    "    return c_tensor, K_tensor, q_tensor, m_ineq, l_tensor, u_tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
