{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqrAapbafBAU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from primal_dual_hybrid_gradient_step import adaptive_one_step_pdhg, fixed_one_step_pdhg\n",
        "from helpers import spectral_norm_estimate_torch, KKT_error, compute_residuals_and_duality_gap, check_termination\n",
        "from enhancements import primal_weight_update\n",
        "\n",
        "def pdlp_algorithm(K, m_ineq, c, q, l, u, device, max_iter=100_000, tol=1e-4, verbose=True, restart_period=40, precondition=False, primal_update=False, adaptive=False, data_precond=None):\n",
        "    '''\n",
        "    Main PDLP algorithm implementation with integrated infeasibility detection.\n",
        "    Args:\n",
        "        K (torch.Tensor): Constraint matrix.\n",
        "        m_ineq (int): Number of inequality constraints.\n",
        "        c (torch.Tensor): Coefficients for the primal objective.\n",
        "        q (torch.Tensor): Right-hand side vector for the constraints.\n",
        "        l (torch.Tensor): Lower bounds for the primal variable.\n",
        "        u (torch.Tensor): Upper bounds for the primal variable.\n",
        "        device (torch.device): Device to run the algorithm on (CPU or GPU).\n",
        "        max_iter (int): Maximum number of iterations.\n",
        "        tol (float): Tolerance for convergence.\n",
        "        verbose (bool): Whether to print detailed output.\n",
        "        restart_period (int): Number of iterations between restarts.\n",
        "        precondition (bool): Whether to use preconditioning.\n",
        "        primal_update (bool): Whether to perform primal weight updates.\n",
        "        adaptive (bool): Whether to use adaptive stepsize.\n",
        "        data_precond (tuple): Preconditioned data (D_col, D_row, K_unscaled, c_unscaled, q_unscaled, l_unscaled, u_unscaled) if precondition is True.\n",
        "\n",
        "    Returns:\n",
        "        x (torch.Tensor): Optimal primal variable.\n",
        "        prim_obj (float): Optimal primal objective value.\n",
        "        k (int): Total number of iterations.\n",
        "        n (int): Number of restart loops.\n",
        "        j (int): KKT pass counter.\n",
        "    '''\n",
        "    # Recover G, A, h, b for infeasibility detection\n",
        "    m_eq = K.shape[0] - m_ineq\n",
        "    if m_ineq > 0:\n",
        "        G_mat = K[:m_ineq]\n",
        "        h = q[:m_ineq]\n",
        "    else:\n",
        "        G_mat = torch.zeros((0, K.shape[1]), device=device)\n",
        "        h = torch.zeros((0, 1), device=device)\n",
        "    if m_eq > 0:\n",
        "        A_mat = K[m_ineq:]\n",
        "        b = q[m_ineq:]\n",
        "    else:\n",
        "        A_mat = torch.zeros((0, K.shape[1]), device=device)\n",
        "        b = torch.zeros((0, 1), device=device)\n",
        "\n",
        "    # Dual bound masks\n",
        "    is_neg_inf = torch.isinf(l) & (l < 0)\n",
        "    is_pos_inf = torch.isinf(u) & (u > 0)\n",
        "\n",
        "    l_dual = l.clone()\n",
        "    u_dual = u.clone()\n",
        "    l_dual[is_neg_inf] = 0\n",
        "    u_dual[is_pos_inf] = 0\n",
        "\n",
        "    q_norm = torch.linalg.norm(q, 2)\n",
        "    c_norm = torch.linalg.norm(c, 2)\n",
        "\n",
        "    # Initial step-size\n",
        "    eta = 0.9 / spectral_norm_estimate_torch(K, num_iters=100)\n",
        "    omega = c_norm / q_norm if q_norm > 1e-6 and c_norm > 1e-6 else torch.tensor(1.0)\n",
        "\n",
        "    theta = 1.0\n",
        "\n",
        "    # Restart Parameters [Sufficient, Necessary, Artificial]\n",
        "    beta = [0.2, 0.8, 0.36]\n",
        "\n",
        "    # Initialize primal and dual\n",
        "    x = torch.zeros((c.shape[0], 1), device=device)\n",
        "    y = torch.zeros((K.shape[0], 1), device=device)\n",
        "\n",
        "    # Infeasibility detection parameters\n",
        "    tol_eq = 1e-2\n",
        "    tol_feas = 1e-2\n",
        "    tol_obj = 1e-2\n",
        "    tol_dual_eq = 1e-2\n",
        "    tol_cert = 1e-2\n",
        "\n",
        "    # Trackers for infeasibility detection\n",
        "    x_prev_iter = x.clone()\n",
        "    y_prev_iter = y.clone()\n",
        "    lam_prev = torch.zeros_like(x)\n",
        "    certificate_flag = None\n",
        "    certificate_iter = None\n",
        "\n",
        "    # Counters\n",
        "    n = 0 # Outer Loop Counter\n",
        "    k = 0 # Total Iteration Counter\n",
        "    j = 0 # Kkt pass Counter\n",
        "\n",
        "    # Initialize Previous KKT Error\n",
        "    KKT_first = 0 # The actual KKT error of the very first point doesn't matter since the artificial criteria will always hit anyway\n",
        "\n",
        "    # -------------- Outer Loop --------------\n",
        "    while k < max_iter:\n",
        "        t = 0 # Initialize inner iteration counter\n",
        "\n",
        "        # Initialize/Reset sums for averaging\n",
        "        x_eta_total = torch.zeros_like(x)\n",
        "        y_eta_total = torch.zeros_like(y)\n",
        "        eta_total = 0\n",
        "\n",
        "        # Initialize/Reset Previous restart point for primal weighting\n",
        "        x_last_restart = x.clone()\n",
        "        y_last_restart = y.clone()\n",
        "\n",
        "        # --------- Inner Loop ---------\n",
        "        while k < max_iter:\n",
        "            k += 1\n",
        "            x_previous = x.clone() # For checking necessary criteria\n",
        "            y_previous = y.clone()\n",
        "\n",
        "            if adaptive:\n",
        "                # Adaptive step of pdhg\n",
        "                x, y, eta, eta_hat, j = adaptive_one_step_pdhg(x, y, c, q, K, l, u, m_ineq, eta, omega, theta, k, j)\n",
        "            else:\n",
        "                # Fixed step of pdhg\n",
        "                x, y, eta, eta_hat = fixed_one_step_pdhg(x, y, c, q, K, l, u, m_ineq, eta, omega, theta)\n",
        "                j += 1\n",
        "\n",
        "            # Increase iteration counters\n",
        "            t += 1\n",
        "\n",
        "            # Update totals\n",
        "            x_eta_total += eta * x\n",
        "            y_eta_total += eta * y\n",
        "            eta_total += eta\n",
        "\n",
        "            # Update eta\n",
        "            eta = eta_hat\n",
        "\n",
        "            # Infeasibility detection\n",
        "            lam = c - K.T @ y\n",
        "            if k > 1:  # Need at least two points to compute differences\n",
        "                dx = x - x_prev_iter\n",
        "                dy = y - y_prev_iter\n",
        "                dlam = lam - lam_prev\n",
        "\n",
        "                # Dual infeasibility (primal unbounded) detection\n",
        "                dlam_plus = (-dlam).clamp(min=0)\n",
        "                dlam_minus = dlam.clamp(min=0)\n",
        "\n",
        "                # Check dual infeasibility (primal unbounded)\n",
        "                if m_eq == 0 or (A_mat @ dx).norm() < tol_eq:\n",
        "                    Gdx = G_mat @ dx if m_ineq > 0 else torch.zeros((1, 1), device=device)\n",
        "                    if (m_ineq == 0 or torch.all(Gdx >= -tol_feas)) and (c.T @ dx < tol_obj):\n",
        "                        bounds_ok = True\n",
        "                        for i in range(x.shape[0]):\n",
        "                            dx_i = dx[i].item()\n",
        "                            c_i = c[i].item()\n",
        "                            l_i = l[i].item()\n",
        "                            u_i = u[i].item()\n",
        "                            if not (\n",
        "                                (not torch.isinf(l[i]) and not torch.isinf(u[i]) and abs(dx_i) <= tol_feas) or\n",
        "                                (u_i == float('inf') and c_i >= 0 and dx_i >= -tol_feas) or\n",
        "                                (l_i == -float('inf') and c_i <= 0 and dx_i <= tol_feas)\n",
        "                            ):\n",
        "                                bounds_ok = False\n",
        "                                break\n",
        "                        if bounds_ok:\n",
        "                            certificate_flag = \"DUAL_INFEASIBLE\"\n",
        "                            certificate_iter = k\n",
        "                            if verbose:\n",
        "                                print(f\"[PDLP] Dual infeasibility detected at iter {k}\")\n",
        "                            break\n",
        "\n",
        "                # Primal infeasibility (dual unbounded) detection\n",
        "                dy_in = dy[:m_ineq] if m_ineq > 0 else torch.zeros((0, 1), device=device)\n",
        "                dy_eq = dy[m_ineq:] if m_eq > 0 else torch.zeros((0, 1), device=device)\n",
        "                dual_res = torch.zeros_like(x)\n",
        "                if m_ineq > 0: dual_res += G_mat.T @ dy_in\n",
        "                if m_eq > 0: dual_res += A_mat.T @ dy_eq\n",
        "                dual_res -= dlam\n",
        "\n",
        "                if dual_res.norm() < tol_dual_eq and (m_ineq == 0 or torch.all(dy_in >= -tol_feas)):\n",
        "                    dual_combo = 0.0\n",
        "                    if m_ineq > 0: dual_combo += (h.T @ dy_in).item()\n",
        "                    if m_eq > 0: dual_combo += (b.T @ dy_eq).item()\n",
        "\n",
        "                    finite_l = (~torch.isinf(l).view(-1)) & (l.view(-1) != 0)\n",
        "                    finite_u = (~torch.isinf(u).view(-1)) & (u.view(-1) != 0)\n",
        "\n",
        "                    if finite_l.any():\n",
        "                        dual_combo -= (l[finite_l].view(1, -1) @ dlam_minus[finite_l].view(-1, 1)).item()\n",
        "                    if finite_u.any():\n",
        "                        dual_combo -= (u[finite_u].view(1, -1) @ dlam_plus[finite_u].view(-1, 1)).item()\n",
        "\n",
        "                    if dual_combo > -tol_cert:\n",
        "                        certificate_flag = \"PRIMAL_INFEASIBLE\"\n",
        "                        certificate_iter = k\n",
        "                        if verbose:\n",
        "                            print(f\"[PDLP] Primal infeasibility detected at iter {k}\")\n",
        "                        break\n",
        "\n",
        "            # Update previous iterates for next infeasibility check\n",
        "            x_prev_iter.copy_(x)\n",
        "            y_prev_iter.copy_(y)\n",
        "            lam_prev.copy_(lam)\n",
        "\n",
        "            # Check Restart Criteria Every restart_period iterations\n",
        "            if t % restart_period == 0:\n",
        "                # Compute averages\n",
        "                x_avg = x_eta_total / eta_total\n",
        "                y_avg = y_eta_total / eta_total\n",
        "\n",
        "                # Compute KKT errors\n",
        "                KKT_current = KKT_error(x, y, c, q, K, m_ineq, omega, is_neg_inf, is_pos_inf, l_dual, u_dual, device)\n",
        "                KKT_average = KKT_error(x_avg, y_avg, c, q, K, m_ineq, omega, is_neg_inf, is_pos_inf, l_dual, u_dual, device)\n",
        "                KKT_min = min(KKT_current, KKT_average)\n",
        "                KKT_previous = KKT_error(x_previous, y_previous, c, q, K, m_ineq, omega, is_neg_inf, is_pos_inf, l_dual, u_dual, device)\n",
        "\n",
        "                # Add three kkt passes\n",
        "                j += 3\n",
        "\n",
        "                # Check Restart Criteria and update with Restart Candidate\n",
        "                if KKT_min <= beta[0] * KKT_first: # Sufficient Criteria\n",
        "                    if verbose:\n",
        "                        print(f\"Sufficient restart at iteration {t} using the\",\n",
        "                              \"Average iterate.\" if KKT_current >= KKT_average else \"Current iterate.\")\n",
        "                    (x, y) = (x_avg, y_avg) if KKT_current >= KKT_average else (x, y)\n",
        "                    break\n",
        "                elif KKT_min <= beta[1] * KKT_first and KKT_min > KKT_previous: # Necessary Criteria\n",
        "                    if verbose:\n",
        "                        print(f\"Necessary restart at iteration {t} using the\",\n",
        "                              \"Average iterate.\" if KKT_current >= KKT_average else \"Current iterate.\")\n",
        "                    (x, y) = (x_avg, y_avg) if KKT_current >= KKT_average else (x, y)\n",
        "                    break\n",
        "                elif t >= beta[2] * k: # Artificial Criteria\n",
        "                    if verbose:\n",
        "                        print(f\"Artificial restart at iteration {t} using the\",\n",
        "                              \"Average iterate.\" if KKT_current >= KKT_average else \"Current iterate.\")\n",
        "                    (x, y) = (x_avg, y_avg) if KKT_current >= KKT_average else (x, y)\n",
        "                    break\n",
        "\n",
        "        # ------------- End Inner Loop ------------\n",
        "\n",
        "        # Handle infeasibility certificate\n",
        "        if certificate_flag:\n",
        "            prim_obj = c.T @ x\n",
        "            if verbose:\n",
        "                print(f\"Terminating due to {certificate_flag} at iteration {k}\")\n",
        "            return x, prim_obj.cpu().item(), k, n, j\n",
        "\n",
        "        n += 1 # Increase restart loop counter\n",
        "\n",
        "        if primal_update: # Primal weight update\n",
        "            omega = primal_weight_update(x_last_restart, x, y_last_restart, y, omega, 0.5)\n",
        "\n",
        "        KKT_first = KKT_error(x, y, c, q, K, m_ineq, omega, is_neg_inf, is_pos_inf, l_dual, u_dual, device)\n",
        "        j += 1 # Add one kkt pass\n",
        "\n",
        "        # Compute primal and dual residuals, and duality gap\n",
        "        if precondition:\n",
        "            D_col, D_row, K_unscaled, c_unscaled, q_unscaled, l_unscaled, u_unscaled = data_precond\n",
        "            l_unscaled[is_neg_inf] = 0\n",
        "            u_unscaled[is_pos_inf] = 0\n",
        "            primal_residual, dual_residual, duality_gap, prim_obj, adjusted_dual = compute_residuals_and_duality_gap(\n",
        "                D_col * x, D_row * y, c_unscaled, q_unscaled, K_unscaled, m_ineq,\n",
        "                is_neg_inf, is_pos_inf, l_unscaled, u_unscaled\n",
        "            )\n",
        "        else:\n",
        "            primal_residual, dual_residual, duality_gap, prim_obj, adjusted_dual = compute_residuals_and_duality_gap(\n",
        "                x, y, c, q, K, m_ineq, is_neg_inf, is_pos_inf, l_dual, u_dual\n",
        "            )\n",
        "        j += 1 # Add one kkt pass\n",
        "\n",
        "        if verbose:\n",
        "            rel_gap = duality_gap.item() / (1 + abs(prim_obj.item()) + abs(adjusted_dual.item()))\n",
        "            rel_primal = primal_residual.item() / (1 + q_norm)\n",
        "            rel_dual = dual_residual.item() / (1 + c_norm)\n",
        "\n",
        "            print(f\"[{k}] Primal Obj: {prim_obj.item():.4f}, Adjusted Dual Obj: {adjusted_dual.item():.4f}, \"\n",
        "                  f\"Gap: {rel_gap:.2e}, Prim Res: {rel_primal:.2e}, Dual Res: {rel_dual:.2e}\")\n",
        "            print(\"\")\n",
        "\n",
        "        # Termination conditions\n",
        "        if check_termination(primal_residual, dual_residual, duality_gap, prim_obj, adjusted_dual, q_norm, c_norm, tol):\n",
        "            if verbose:\n",
        "                print(f\"Converged at iteration {k} after {n} restart loops\")\n",
        "            break\n",
        "\n",
        "    # ------------------- End Outer Loop ------------------------\n",
        "\n",
        "    return x, prim_obj.cpu().item(), k, n, j"
      ]
    }
  ]
}